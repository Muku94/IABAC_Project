{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade5d0f-942e-4c36-9cb1-d3f8780fb7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e36ec5f-46af-45f1-b633-72e0ace2a0ad",
   "metadata": {},
   "source": [
    "### INX Future Inc Employee Performance - Project \n",
    "\n",
    "INX Future Inc, (referred as INX) , is one of the leading data analytics and automation solutions provider with over 15 years of global business presence. INX is consistently rated as top 20 best employers past 5 years. INX human resource policies are considered as employee friendly and widely perceived as best practices in the industry. Recent years, the employee performance indexes are not healthy, and this is becoming a growing concern among the top management. There has been increased escalations on service delivery and client satisfaction levels came down by 8 percentage points. CEO, Mr. Brain, knows the issues but concerned to take any actions in penalizing non-performing employees as this would affect the employee morale of all the employees in general and may further reduce the performance. Also, the market perception best employer and thereby attracting best talents to join the company. Mr. Brain decided to initiate a data science project, which analyses the current employee data and find the core underlying causes of this performance issues. Mr. Brain, being a data scientist himself, expects the findings of this project will help him to take right course of actions. He also expects a clear indicator of nonperforming employees, so that any penalization of non-performing employee, if required, may not significantly affect other employee morals. \n",
    "\r\n",
    "The following insights are expected from this project. \r\n",
    "1. Department wise performances \r\n",
    "2. Top 3 Important Factors effecting employee performance\r\n",
    " 3. A trained model which can predict the employee performance based on factors as inputs. This will be used to hire employees. \r\n",
    "4. Recommendations to improve the employee performance based on insights from analysis.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9ef48-8f12-4f43-8eea-f994de1376dd",
   "metadata": {},
   "source": [
    "Summary of project steps\n",
    "\n",
    "1. Data Loading and Preprocessing\n",
    "2. Exploratory Data Analysis\n",
    "3. Data Preprocessing \n",
    "4. Model Building, traning and prediction\n",
    "5. Results and Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d084f55-4917-440c-80ff-f1725d44f56a",
   "metadata": {},
   "source": [
    "Data Understanding\n",
    "The data for training the model was collected from the company INX Future Inc. The data collected included 1200 employee’s performance appraisal records, described by 28 parameters. the parameters show the different attributes of an employee based on which the prediction is to be made.\n",
    "\n",
    "Features present in the dataset\n",
    "1. EmpNumber\n",
    "2. Age\n",
    "3. Gender\n",
    "4. EducationBackground\n",
    "5. MaritalStatus\n",
    "6. EmpDepartment\n",
    "7. EmpJobRole\n",
    "8. BusinessTravelFrequency\n",
    "9. DistanceFromHome\n",
    "10. EmpEducationLevel\n",
    "11. EmpEnvironmentSatisfaction\n",
    "12. EmpHourlyRate\n",
    "13. EmpJobInvolvement\n",
    "14. EmpJobLevel\n",
    "15. EmpJobSatisfaction\n",
    "16. NumCompaniesWorked\n",
    "17. OverTime\n",
    "18. EmpLastSalaryHikePercent\n",
    "19. EmpRelationshipSatisfaction\n",
    "20. TotalWorkExperienceInYears\n",
    "21. TrainingTimesLastYear\n",
    "22. EmpWorkLifeBalance\n",
    "23. ExperienceYearsAtThisCompany\n",
    "24. ExperienceYearsInCurrentRole\n",
    "25. YearsSinceLastPromotion\n",
    "26. YearsWithCurrManager\n",
    "27. Attrition\n",
    "28. PerformanceRating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fca22a-18db-493b-81c0-ab27da0fe3ef",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731e581-d64f-4230-a8dd-904c522e17da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e81e7-c64d-49dc-972b-165a4c6fca03",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2e024-7035-48a4-aa1e-28db7e65c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Excel data file and df\n",
    "\n",
    "df = pd.read_excel(\"INX_Future_Inc_Employee_Performance_CDS_Project2_Data_V1.8.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08c656-e413-4e8a-8c44-86422664ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the first five rows \n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088ebad-2d54-4f3b-a701-c8c4e5a0f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of rows and columns of the dataset\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222afd5-8f5a-4095-a336-f6e5c927f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an object with all of the column names\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99978b4-24cd-456f-8b48-be67f5fba969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns basic information on all columns (columnName, Non-nullCount & Dtyyes)\n",
    "\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3842d91-9202-416e-bcc4-49c65c22b9ed",
   "metadata": {},
   "source": [
    "\n",
    "*The dataset has a total of 28 features and 1200 instances. Among these features, 19 are of the int64 data type, while the remaining 9 features are of the object data type.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f74812-be26-4e85-ad41-b860f94440d9",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d113c-2d63-4d25-abe3-0840552d0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns basic statistics on numeric columns\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57803e-2aae-4238-a98c-7b9153b3850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a summary of statistics on categorical columns\n",
    "\n",
    "df.describe(include=\"O\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b339a1-1789-4c49-af36-fb0b64e2e3da",
   "metadata": {},
   "source": [
    "### Unviriate analysis, Biaviriate analysis and Multviriate analysis\n",
    "\n",
    "**Discrete and Categorical**\n",
    "\n",
    "histogram: A histogram is a bar graph-like representation of data that buckets a range of classes into columns along the horizontal x-axis. The vertical y-axis represents the number count or percentage of occurrences in the data for each column.\n",
    "line plots: A-Line plot can be defined as a graph that displays data as points or check marks above a number line, showing the frequency of each value.\n",
    "\n",
    "**continuous features**\n",
    "\n",
    "Countplot: counterplot is used to show the counts of observations in each categorical bin using bars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c8bb4-ef41-4e2c-a4c6-078b6f54a4b3",
   "metadata": {},
   "source": [
    "### Unviriate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d982ba55-11cb-446d-9bcf-d5d40fd262eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that takes the column name and data frame returning a visualization\n",
    "\n",
    "def plot_histograms(df, columns, figsize=(15, 10), fontsize=12, bins='auto', color='y'):\n",
    "    \"\"\"\n",
    "    This function plots histograms for specified columns in the data using subplots,\n",
    "    choosing an appropriate plot type based on the data type.\n",
    "    \"\"\"\n",
    "    n_cols = 2  # Number of columns per row for the subplot\n",
    "    n_rows = (len(columns) + n_cols - 1) // n_cols  # Ensure enough rows for the number of columns\n",
    "    \n",
    "    # Create figure with determined rows and columns\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, squeeze=False)\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy looping\n",
    "    \n",
    "    # Loop through the specified columns and plot appropriately\n",
    "    for idx, column in enumerate(columns):\n",
    "        if column in df.columns:\n",
    "            # Check data type and choose the plot type\n",
    "            if df[column].dtype.kind in 'bifc':  # Numeric types\n",
    "                sns.histplot(df[column], ax=axes[idx], bins=bins, color=color, kde=True)\n",
    "            else:  # Assume categorical type\n",
    "                sns.countplot(x=df[column], ax=axes[idx], color=color)\n",
    "                \n",
    "            axes[idx].set_xlabel(column, fontsize=fontsize)\n",
    "            axes[idx].set_ylabel('Frequency', fontsize=fontsize)\n",
    "            axes[idx].set_title(f'Histogram of {column}', fontsize=fontsize)\n",
    "        else:\n",
    "            print(f\"Column {column} not found in the dataset.\")\n",
    "            axes[idx].set_visible(False)  # Hide axis if column is not present\n",
    "    \n",
    "    # Adjust the layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.4)  # Adjust horizontal and vertical spacing\n",
    "    plt.show()\n",
    "\n",
    "# Sample DataFrame creation\n",
    "data =df\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the columns to plot\n",
    "columns = ['Age', 'Gender', 'EducationBackground', 'MaritalStatus', 'EmpDepartment', 'Attrition']\n",
    "\n",
    "# Call the function\n",
    "plot_histograms(df, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728b4dd-df11-463c-9232-cc9bb4504fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulizing EmpJobRole\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(data.EmpJobRole)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('EmpJobRole',fontsize=20)\n",
    "plt.title('EmpJobRole',fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a8c77-2435-414b-ad74-13330bab5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_counts = df['EmpDepartment'].value_counts()\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.pie(department_counts, labels=department_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Employee Department Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40738bf-079b-4752-9165-10aade2f3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating subplots to visualize indpendent colunms\n",
    "\n",
    "list_col_names= df[['BusinessTravelFrequency','DistanceFromHome',\n",
    "              'EmpEducationLevel', 'EmpEnvironmentSatisfaction','EmpJobInvolvement', 'EmpJobLevel',\n",
    "              'EmpJobSatisfaction', 'NumCompaniesWorked', 'OverTime']] # making lsit of next 12 feature\n",
    "\n",
    "plt.figure(figsize=(20,25)) # defining canvas size\n",
    "plotno = 1 # counter\n",
    "\n",
    "for column in list_col_names: # iteration of columns / acessing the columns from count \n",
    "    if plotno<=13:    # set the limit\n",
    "        plt.subplot(4,3,plotno) # # plotting 12 graphs (4-rows,3-columns) ,plotnumber is for count\n",
    "        sns.countplot(x=list_col_names[column]) # Plotting count plots because the feature data type is discerte and categorical\n",
    "        plt.xlabel(column,fontsize=20)  # assigning name to x-axis and font size is 20\n",
    "    plotno+=1 # counter increment\n",
    "plt.tight_layout()\n",
    "plt.show() # used to hide the storage loction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37df9d3-854d-4197-b846-c1a9366ae750",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_names2 = df[['EmpLastSalaryHikePercent', 'EmpRelationshipSatisfaction','TrainingTimesLastYear','EmpWorkLifeBalance',\n",
    "               'ExperienceYearsInCurrentRole', 'YearsSinceLastPromotion','YearsWithCurrManager', 'Attrition', \n",
    "               'PerformanceRating']]  #Making nex feature list to plot the graph\n",
    "\n",
    "plt.figure(figsize=(20,22)) # defining canvas size\n",
    "plotno = 1 # counter\n",
    "\n",
    "for column in list_col_names2: # iteration of columns / acessing the columns from count \n",
    "    if plotno<=10:    # set the limit\n",
    "        plt.subplot(3,3,plotno) # # plotting 9 graphs (3-rows,3-columns) ,plotnumber is for count\n",
    "        sns.countplot(x=list_col_names2[column]) # Plotting count plots because the feature data type is discerte and categorical\n",
    "        plt.xlabel(column,fontsize=20)  # assigning name to x-axis and font size is 20\n",
    "    plotno+=1 # counter increment\n",
    "plt.tight_layout()\n",
    "plt.show() # used to hide the storage loction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0848b-6ac8-4301-bc27-c75047ca9744",
   "metadata": {},
   "source": [
    "#### Key findings from data exploration analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624c033-83b5-4252-991b-3afbfe079406",
   "metadata": {},
   "source": [
    "#### Summary for specific features \n",
    "cen fewer at level 4.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e95c1-c234-4ba2-8ff4-fb7dbe7b2ad1",
   "metadata": {},
   "source": [
    "### Summary key insights "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b4f3a19-706e-4b45-93f3-f553db87c555",
   "metadata": {},
   "source": [
    "\n",
    "Demographics and Stability:\n",
    "The workforce is primarily male and predominantly consists of married individuals. Most employees have a stable relationship with the company, evidenced by low attrition rates and a majority not having switched too many companies.\n",
    "\n",
    "Work Conditions and Satisfaction:\n",
    "Employees generally exhibit high satisfaction levels regarding their work environment and job responsibilities. The majority have moderate to high job involvement and satisfaction, which is crucial for long-term employee retention and productivity.\n",
    "\n",
    "Professional Development:\n",
    "Employees mostly have mid to high levels of education and receive regular training, suggesting an emphasis on continuous professional development. However, promotions do not seem very frequent, which might be an area to explore for enhancing motivation and retention.\n",
    "\n",
    "Geographic and Logistic Factors:\n",
    "A significant number of employees live close to the workplace, which could contribute to higher job satisfaction and lower daily stress related to commuting.\n",
    "\n",
    "Diversity in Educational Backgrounds:\n",
    "While there is a concentration in fields like Life Sciences, Medical, and Marketing, there is a diversity of educational backgrounds, which could be leveraged for innovative strategies and cross-functional projects.\n",
    "\n",
    "Work-life Balance and Overtime:\n",
    "Most employees seem to have a good work-life balance, with a substantial number not required to work overtime, supporting employee well-being and possibly contributing to the low attrition rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03f185-c2bb-45bc-8000-b55fd7920f7a",
   "metadata": {},
   "source": [
    "### Biaviriate analysis\n",
    "\n",
    "Plot used:\r\n",
    "\r\n",
    "Histogram, line plot used for continuous features Count used for categorical & Discrete datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d14c3a-47a5-453f-8016-bdb45165ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This code creats a 3x2 grid of line plots using Python's Matplotlib and Seaborn\n",
    "to analyze relationships between various employment metrics in a DataFrame'''\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))  # Adjust the size as needed\n",
    "\n",
    "# Top left plot\n",
    "sns.lineplot(x='Age', y='TotalWorkExperienceInYears', data=df, ax=axs[0, 0])\n",
    "axs[0, 0].set_xlabel('Age', fontsize=15)\n",
    "axs[0, 0].set_title('Total Work Experience vs. Age')\n",
    "\n",
    "# Top right plot\n",
    "sns.lineplot(x='ExperienceYearsAtThisCompany', y='TotalWorkExperienceInYears', data=df, ax=axs[0, 1])\n",
    "axs[0, 1].set_xlabel('Experience Years At This Company', fontsize=15)\n",
    "axs[0, 1].set_title('Work Experience vs. Experience at Current Company')\n",
    "\n",
    "# Middle left plot\n",
    "sns.lineplot(x='EmpLastSalaryHikePercent', y='NumCompaniesWorked', data=df, ax=axs[1, 0])\n",
    "axs[1, 0].set_xlabel('Emp Last Salary Hike Percent', fontsize=15)\n",
    "axs[1, 0].set_title('Number of Companies Worked vs. Last Salary Hike')\n",
    "\n",
    "# Middle right plot\n",
    "sns.lineplot(x='YearsSinceLastPromotion', y='ExperienceYearsInCurrentRole', data=df, ax=axs[1, 1])\n",
    "axs[1, 1].set_xlabel('Years Since Last Promotion', fontsize=15)\n",
    "axs[1, 1].set_title('Experience in Current Role vs. Years Since Last Promotion')\n",
    "\n",
    "# Bottom left plot\n",
    "sns.lineplot(x='YearsSinceLastPromotion', y='ExperienceYearsInCurrentRole', data=df, ax=axs[2, 0])\n",
    "axs[2, 0].set_xlabel('Years Since Last Promotion', fontsize=15)\n",
    "axs[2, 0].set_title('Experience in Current Role vs. Years Since Last Promotion')\n",
    "\n",
    "# Bottom right plot\n",
    "sns.lineplot(x='DistanceFromHome', y='EmpLastSalaryHikePercent', data=df, ax=axs[2, 1])\n",
    "axs[2, 1].set_xlabel('Distance From Home', fontsize=15)\n",
    "axs[2, 1].set_title('Last Salary Hike Percent vs. Distance From Home')\n",
    "\n",
    "plt.tight_layout()  # Adjust subplots to fit into figure area.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99ab43-a2f1-479b-be9f-13fa6aeac191",
   "metadata": {},
   "source": [
    "#### Summary key features relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985ea77-9e36-4e2c-aa7c-d0cfb84873da",
   "metadata": {},
   "source": [
    "1.\tAge and Experience Correlation:\n",
    "2.\tEmployees aged 18-25 usually have less than 5 years of experience, indicating that total work experience increases with age.\n",
    "3.\tCompany Tenure and Total Experience:\n",
    "4.\tThe longer an employee works at the same company, the greater their overall work experience, highlighting the accumulation of expertise over time.\n",
    "5.\tNumber of Previous Employers and Recent Salary Hikes:\n",
    "6.\tEmployees who have worked for fewer companies tend to receive larger salary increases, possibly reflecting a reward for loyalty or stability.\n",
    "7.\tPromotions and Experience in Current Role:\n",
    "8.\tMore years since the last promotion typically means more experience in the current role, suggesting employees might not quickly ascend to higher positions despite their expertise.\n",
    "9.\tHourly Rate and Tenure with Current Manager:\n",
    "10.\tWhile most employees' hourly rates are consistent regardless of their tenure with a current manager, some achieve high rates quickly, possibly due to performance or negotiation.\n",
    "11.\tCommute Distance and Salary Hikes:\n",
    "12.\tLonger commuting distances are often compensated with higher salary hikes, but there’s a specific dip in hikes for commutes between 10 to 13 miles, indicating possible local influences on compensation policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e5549-dffa-4585-a202-c56251ea61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code generates a bar plot that displays the counts of employees within each department, categorized by performance rating\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Choose a diverse color palette\n",
    "palette = sns.color_palette(\"viridis\", n_colors=df['PerformanceRating'].nunique())\n",
    "\n",
    "# Create a countplot\n",
    "ax = sns.countplot(x='EmpDepartment', hue='PerformanceRating', data=df, palette=palette)\n",
    "\n",
    "# Annotate each bar with the count\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 20), \n",
    "                textcoords = 'offset points')\n",
    "\n",
    "# Improve plot aesthetics with larger font sizes for better readability\n",
    "ax.set_title('Employee Departments vs Performance Ratings', fontsize=35) \n",
    "ax.set_xlabel('Employee Department', fontsize=25)  \n",
    "ax.set_ylabel('Number of Employees', fontsize=25)  \n",
    "ax.legend(title=\"Performance Rating\", title_fontsize='16', fontsize='14')  # Increased legend font size\n",
    "\n",
    "# Adjust the x-axis labels to avoid overlapping and ensure they are readable\n",
    "plt.xticks(rotation=45, fontsize=18)  \n",
    "plt.yticks(fontsize=18)  # Increase y-axis ticks font size\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39696bf-a7c0-4ff4-a1fb-a621d9e2b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_performance = df.groupby('EmpDepartment')['PerformanceRating'].mean().reset_index()\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 7))  # Adjust size to better fit a pie chart\n",
    "\n",
    "# Creating the pie chart\n",
    "pie_wedges = plt.pie(department_performance['PerformanceRating'], \n",
    "                     labels=department_performance['EmpDepartment'], \n",
    "                     autopct='%1.1f%%', startangle=140, \n",
    "                     colors=plt.cm.viridis(np.linspace(0, 1, len(department_performance['EmpDepartment']))))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Average Performance Rating by Department', fontsize=24, color='black')\n",
    "\n",
    "# Change the font color of labels and autopct to white\n",
    "for text in pie_wedges[1]:\n",
    "    text.set_color('black')\n",
    "for autotext in pie_wedges[2]:\n",
    "    autotext.set_color('r')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.axis('equal')  \n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6e549-40ad-4d76-af8e-cd3ea9648d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This  code calculate the average performance ratings of employees, grouped by department and gender.\n",
    "\n",
    "average_ratings = df.groupby(['EmpDepartment', 'Gender'])['PerformanceRating'].mean().unstack()\n",
    "\n",
    "# Create a bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "average_ratings.plot(kind='bar', ax=ax, color=['#1f77b4', '#ff7f0e'])  # Two colors for male and female\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Employee Department', fontsize=16)\n",
    "ax.set_ylabel('Average Performance Rating', fontsize=16)\n",
    "ax.set_title('Average Performance Rating by Department and Gender', fontsize=18)\n",
    "ax.legend(title='Gender', fontsize=12)\n",
    "\n",
    "\n",
    "# Customize the tick labels for better readability\n",
    "ax.set_xticklabels(average_ratings.index, rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f05cf-9b17-4f98-b6fe-3ae96a261202",
   "metadata": {},
   "source": [
    "### Key Factors: Department-Wise Performance\n",
    "#### Data insights show the following:\n",
    "\n",
    "1. DataScience: The highest average of level 3 performance is in data science department. Data science is the only department with less number of level 2 performers. The overall performance is higher compared to all departments. Male employees are doing good in this department.\n",
    "2. Development:The maximum number of employees are level 3 performers.The gender-based performance is nearly same for both. Development depertment has the highiest number of level 4 performers.\n",
    "3. Finance: The male employees are doing good. It is also observed that Finance depertmenrt is the least performing by average\n",
    "4. Human Resource:The majority of the employees lying under the level 3 performance . The female employees in HR department doing really well in their performance.\n",
    "5. Research and develpment: The Reserch and develpment has the good female employees in their performance.\n",
    "6. sales: The Performace rating level 3 is more in the sales department. The male performance rating the little bit higher compared to female."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f8524-3a88-469a-8728-6e8364e0c486",
   "metadata": {},
   "source": [
    "### Multiviriate analysis\n",
    "\n",
    "Checking the relationship between two features with respect to target features\n",
    "\n",
    "PLOT USED:\n",
    "\n",
    "Line plot: A Line plot can be defined as a graph that displays data as points or check marks above a number line, showing the frequency of each value. Barplot: It shows the relationship between a numeric and a categoric variable. Each entity of the categoric variable is represented as a bar. The size of the bar represents its numeric value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e68c1-80e2-41bb-b63e-7cff5951895c",
   "metadata": {},
   "source": [
    "#### 1.Check the relationship between Age & Total work experiance with respect to target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85523591-e78c-4fca-846c-0615066f2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot shows the relationship between Age & Total work experiance with respect to target feature\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.lineplot(x='Age',y='TotalWorkExperienceInYears',hue=data.PerformanceRating,data=df)\n",
    "plt.xlabel('Age',fontsize=20)\n",
    "plt.ylabel('TotalWorkExperienceInYears',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e60f52-00cc-4844-96be-7fdaf0fec81a",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "If the age & total experiance is increases that time most of the employee performance rating is 4.\n",
    "Low age and minimum year experiance employee also 4 perfomace rating.\n",
    "At the age of 50 and 30 years of experiance employee have 2 performance rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1647fa55-ee31-431f-9aef-2d256e25b671",
   "metadata": {},
   "source": [
    "### 2. Check the relationship between Gender & Number Companies Worked with respect to target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4679c39-8ada-4954-b889-b5910991d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the relationship between Gender & Number Companies Worked with respect to target feature\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='Gender',y='NumCompaniesWorked',hue=data.PerformanceRating,data=data)\n",
    "plt.xlabel('Gender',fontsize=20)\n",
    "plt.ylabel('NumCompaniesWorked',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9326d-3ce8-41ce-b42c-62294af0ec16",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "If the gender is male and no of companies work is increses that time most of the performance rating is 2 & 4.\n",
    "If female work in less no companies that time performance rating is 2.\n",
    "3 & 4 performance rating in female increase if the no of compaines worked is increses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a9c1f-8f92-4317-915a-69de4aae68ab",
   "metadata": {},
   "source": [
    "### 3. Check the relationship between MaritalStatus & EmpLastSalaryHikePercent with respect to target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ed7c1-7dfa-45bf-8b9b-62e90f4f1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the relationship between MaritalStatus & EmpLastSalaryHikePercent with respect to target feature\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='MaritalStatus',y='EmpLastSalaryHikePercent',hue=data.PerformanceRating,data=data)\n",
    "plt.xlabel('MaritalStatus',fontsize=20)\n",
    "plt.ylabel('EmpLastSalaryHikePercent',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a610fdfe-272e-46ce-bd99-aca6b2e4b50b",
   "metadata": {},
   "source": [
    "Observaton:\n",
    "\n",
    "In all marital status with maximum salary hike percent in last year is increases that time performance rating is 4.\n",
    "If salary hike percentage in last year is less than 15 that time performance rating is 2 & 3 in all marital status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b6535-85c4-42d9-a845-5bb841b6260c",
   "metadata": {},
   "source": [
    "### 4. Check the relationship between EducationBackground & ExperienceYearsInCurrentRole with respect to target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d86fba-1632-45de-8f1e-511c8ba03570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the relationship between EducationBackground & ExperienceYearsInCurrentRole with respect to target feature\n",
    "\n",
    "\n",
    "# Define the blue palette\n",
    "blue_palette = sns.color_palette('Blues')\n",
    "\n",
    "# Data loading and plotting code, example with a barplot\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='EducationBackground', y='ExperienceYearsInCurrentRole', hue='PerformanceRating', data=data, palette=blue_palette)\n",
    "plt.xlabel('EducationBackground', fontsize=20)\n",
    "plt.ylabel('ExperienceYearsInCurrentRole', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1995f-0789-46f3-b50d-663027c5bfdd",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "If the current role experiance is increses of marketing education background employee has 4 performance rating most of the time\n",
    "In all education background with current role experiance is more than 5 then the performance rating of employee is 2.\n",
    "Life scineces, Medical, others , and technical degree education background employee has less than 4 year of experiance in current role that time performance rating is 3 & 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b12af7-8245-4bd2-8de9-9b46072b97f0",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332736d5-4a4e-4194-abea-c95b733bd8cf",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffdcad-d0de-4c16-8ce3-7ad4c0415c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump, load\n",
    "from scipy.stats import boxcox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f5d1c-736e-4b71-86fd-f6a617e8d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values to ensure that missing values are handled before modeling our data\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a91ea6-ebc5-45b1-862d-ed6ce32d5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicate values \n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19970219-3add-4d27-9adb-d35d4cc17fed",
   "metadata": {},
   "source": [
    "#### Creating filter list for categorical and numerical column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25754788-2eaa-43e2-bfa3-6ee85a4f14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating numerical variables from categorical variables; this will allow performing specific to data types... like encording\n",
    "\n",
    "# Creating the list using list comprehension to filter data in the data frame by data type returning a list of dataframe colunm names \n",
    "Cat_cols =[i for i in df.columns if df[i].dtype==\"object\" ]\n",
    "numerical_cols = [i for i in df.columns if df[i].dtype!=\"object\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bc90f-56e2-4272-8880-abb70916aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a72153-d9d1-4a25-879f-6dacb737e033",
   "metadata": {},
   "source": [
    "#### Handling categorical colunms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0662a-a23a-4521-aeca-f8735dd4ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting categorical virables to numerical variables with the help of requency encoder \n",
    "\n",
    "def frequency_encoding(dataframe, categorical_vals, save_path=None):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in a DataFrame based on the frequency of each category's occurrence.\n",
    "    The function calculates the frequency of each unique value in the specified categorical columns\n",
    "    and replaces the categorical value with its frequency. It can also serialize and save the \n",
    "    mapping dictionary for later use using joblib.\n",
    "        DataFrame with categorical values replaced by frequencies.\n",
    "    \"\"\"\n",
    "    # Copying the dataframe to avoid modifying the original data\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Dictionary to hold frequency mappings for each categorical column\n",
    "    mappings = {}\n",
    "\n",
    "    # Process each column specified in categorical_vals\n",
    "    for column in categorical_vals:\n",
    "        # Compute frequency of each value in the column\n",
    "        freq = dataframe[column].value_counts(normalize=True)\n",
    "        # Map frequencies to the categorical values in the column\n",
    "        dataframe[column] = dataframe[column].map(freq)\n",
    "        # Store the computed frequencies in the mappings dictionary\n",
    "        mappings[column] = freq.to_dict()\n",
    "    \n",
    "    # Optionally save the mappings to a file if a path is provided\n",
    "    if save_path:\n",
    "        dump(mappings, save_path)\n",
    "    \n",
    "    # Return the modified dataframe with categories replaced by their frequencies\n",
    "    return dataframe\n",
    "\n",
    "# Example usage\n",
    "df_2 = frequency_encoding(df, Cat_cols, save_path='frequency_mappings.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719ffb0-673c-4fc6-8c78-345b6281b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking df_2 data frame\n",
    "\n",
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60940ca-223a-4ab8-b1a8-520290cb9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b081d-c8d0-461d-877a-b75973d71803",
   "metadata": {},
   "source": [
    "All the colunme have been converted to numerical values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1f42f-8701-4a1b-93b0-8b1be2c229be",
   "metadata": {},
   "source": [
    "#### Handling numerical columns and detecting outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dc44e-072a-4ae5-951f-9f15c4b3bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a viaual plot to inspect posible outliers in all the numerical columns\n",
    "\n",
    "def plot_boxplots(data, columns):\n",
    "    \"\"\"\n",
    "    This functiony creates and displays boxplots for specified numerical columns \n",
    "    in a DataFrame to visually inspect for outliers.\n",
    "    The function directly displays the boxplot figures.\n",
    "    \"\"\"\n",
    "    # Calculate the number of rows needed for the subplots based on the number of columns\n",
    "    num_columns = len(columns)\n",
    "    num_rows = (num_columns + 2) // 3  # Adjusting to have 3 columns per row\n",
    "    \n",
    "    plt.figure(figsize=(20, num_rows * 6))  # Adjust the figure size dynamically based on the number of rows\n",
    "    for i, column in enumerate(columns, 1):\n",
    "        plt.subplot(num_rows, 3, i)  # Adjust the grid definition dynamically\n",
    "        sns.boxplot(x=data[column])  # Clarifying the orientation of the boxplot\n",
    "        plt.xlabel(column, fontsize=20)  # Setting the font size for better readability\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "selected_columns = numerical_cols # assigning selected_columns to numerical_cols\n",
    "plot_boxplots(df_2, selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74eca6f-5ac4-443a-bbed-df4bb3495cd6",
   "metadata": {},
   "source": [
    "#### Visual inspection has revealed the presence of outliers in five columns of the dataset.\n",
    "1. YearsSinceLastPromotion\n",
    "2. ExperienceYearsAtThisCompany\n",
    "3. TotalWorkExperienceInYears\n",
    "4. YearsWithCurrManager\n",
    "5. ExperienceYearsInCurrentRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710b24e-2e1a-423b-84c9-e77777ea94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulising the list of selected colunms to be computed for outliers\n",
    "\n",
    "# defining the list of columns with outliers \n",
    "\n",
    "Outlier_col_list = ['YearsSinceLastPromotion','ExperienceYearsAtThisCompany',\n",
    "                    'TotalWorkExperienceInYears','YearsWithCurrManager','ExperienceYearsInCurrentRole']\n",
    "# ploting the colunms using the plot_boxplots function defined above passing df_2 and Outlier_col_list\n",
    "plot_boxplots(df_2, Outlier_col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba469299-536d-4078-bf0c-6f24839635eb",
   "metadata": {},
   "source": [
    "####  Visualizing the Data Distribution to Determine the Appropriate Method for Outlier Imputation\n",
    "\r\n",
    "To effectively manage outliers, it is crucial to understand the data distribution in order to choose the appropriate method for imputation. This understanding can often be achieved through visualizations that highlight the underlying patterns and potential anomalies in the data. By examining these visual distributions, we can make informed decisions about the most suitable techniques for handling outliers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b1aa1-ae1a-4cd1-a3f0-8f6077ccd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulizing the data distribution of data to determine which method to be used for outlier imputation\n",
    "\n",
    "# Define the list of numeric features\n",
    "numeric_features = Outlier_col_list\n",
    "#creating a series of Kernel Density Estimate (KDE) plots for numeric features defined above \"numeric_features\" and organizing them into a grid layout for easy comparison\n",
    "\n",
    "num_rows = (len(numeric_features) + 1) // 2  # Adding 1 to round up if there's an odd number of features\n",
    "\n",
    "# Set up the figure and axes for plotting\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=2, figsize=(12, 4*num_rows))\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over each numeric feature and plot its KDE\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    sns.histplot(df_2[feature], ax=axes[i], kde=True, element='bars', stat='density')\n",
    "    axes[i].set_title(f' {feature}')\n",
    "\n",
    "# Hide any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2438188-7852-4ebf-921e-3a2aaf740d94",
   "metadata": {},
   "source": [
    "From the visualizations presented, it is evident that most columns exhibit a right-skewed distribution; therefore, ***median*** imputation will be employed to address outliers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9622c2a-84b4-49d6-a278-ade46f99dfe5",
   "metadata": {},
   "source": [
    "#### Outlier Imputation using median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44e4a5-5a72-4d72-92af-92a80e478be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that takes data frame and list of colunm names with outliers \n",
    "\n",
    "def impute_outliers_with_median(data, columns):\n",
    "    \"\"\"\n",
    "    This function Imputes outliers in specified columns of a DataFrame using the median value.\n",
    "    The function returns a DataFrame with outliers imputed using the median value.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    data = data.copy()\n",
    "    for column in columns:\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        median_value = data[column].median()\n",
    "\n",
    "        # Apply imputation to the data values outside the IQR\n",
    "        data.loc[(data[column] < lower_bound) | (data[column] > upper_bound), column] = median_value\n",
    "    \n",
    "    return data\n",
    "\n",
    "# assigning the data frame to imputed data_frame\n",
    "df_imputed = impute_outliers_with_median(df_2, Outlier_col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7086d-b813-436e-b5ad-f8ef6468a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the changes in the imputed columns using the plot_boxplots function defined above\n",
    "\n",
    "plot_boxplots(df_imputed, Outlier_col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394656e-86df-4cc1-a73a-7a9903ed5588",
   "metadata": {},
   "source": [
    "#### Checking for skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068d975-0855-43d3-95a9-ca3f6a81b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and summarizes the skewness and kurtosis for specified numerical columns in a data frame.\n",
    "\n",
    "columns_of_interest = numerical_cols\n",
    "# Initialize a dictionary to store skewness and kurtosis values\n",
    "statistics_data = {}\n",
    "\n",
    "for column_name in columns_of_interest:\n",
    "    skew_value = df_imputed[column_name].skew()\n",
    "    kurtosis_value = df_imputed[column_name].kurtosis()\n",
    "    statistics_data[column_name] = {'Skewness': skew_value, 'Kurtosis': kurtosis_value}\n",
    "    \n",
    "'''This function takes all the numerical and uses the .skew and .kurtosis to check for skewness \n",
    "and kurtosis per feature; it returns a data frame with \n",
    "Loop through each column, calculate skewness and kurtosis, and store them'''\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better visualization\n",
    "stats_df = pd.DataFrame(statistics_data).T  # Transpose to swap rows and columns for better visualization\n",
    "stats_df.index.name = 'Feature'\n",
    "stats_df.reset_index(inplace=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df379217-8b11-43d2-856f-33a94609ab33",
   "metadata": {},
   "source": [
    "### Highly Skewed Features:\n",
    "\n",
    "***EmpJobLevel: Skewness = 1.024, Kurtosis = 0.386***\n",
    "Moderately high skewness might require transformation or outlier treatment.\n",
    "\n",
    "***YearsSinceLastPromotion: Skewness = 1.667, Kurtosis = 1.923***\n",
    "High skewness and kurtosis indicate a significant number of outliers and a sharp peak. This feature is a prime candidate for data transformation to reduce right skewness and manage outliers.\n",
    "\n",
    "***ExperienceYearsAtThisCompany: Skewness = 1.088, Kurtosis = 1.006***\n",
    "Similar to \"YearsSinceLastPromotion\", this feature shows both high skewness and kurtosis, suggesting the need for transformations to normalize the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885d5c5-41d4-4a1f-bc3c-367a9b24a8bc",
   "metadata": {},
   "source": [
    "#### Imputing skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b359718-26b6-4403-ad2d-66ba8d3863d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"\"\"\n",
    "    Applies log transformation followed by Box-Cox transformation to specified columns\n",
    "    in the DataFrame to normalize their distributions. Original columns are dropped \n",
    "    after transformation to prevent multicollinearity in the dataset. The function \n",
    "    modifies the DataFrame in-place and does not return a value.\n",
    "    \"\"\"\n",
    "\n",
    "# Check for zeros in the dataset for log transformations\n",
    "print(\"Zeroes in 'YearsSinceLastPromotion':\", (df_imputed['YearsSinceLastPromotion'] == 0).sum())\n",
    "print(\"Zeroes in 'ExperienceYearsAtThisCompany':\", (df_imputed['ExperienceYearsAtThisCompany'] == 0).sum())\n",
    "\n",
    "# Applying log transformation, adding 1 to avoid log(0)\n",
    "df_imputed['YearsSinceLastPromotion_log'] = np.log1p(df_imputed['YearsSinceLastPromotion'])\n",
    "df_imputed['ExperienceYearsAtThisCompany_log'] = np.log1p(df_imputed['ExperienceYearsAtThisCompany'])\n",
    "\n",
    "# Check if Box-Cox transformation is needed and apply it\n",
    "# Ensure all values are positive for Box-Cox transformation\n",
    "if (df_imputed['YearsSinceLastPromotion_log'] <= 0).any():\n",
    "    df_imputed['YearsSinceLastPromotion_log'] = boxcox(df_imputed['YearsSinceLastPromotion_log'] + 1)[0]\n",
    "else:\n",
    "    df_imputed['YearsSinceLastPromotion_log'] = boxcox(df_imputed['YearsSinceLastPromotion_log'])[0]\n",
    "\n",
    "if (df_imputed['ExperienceYearsAtThisCompany_log'] <= 0).any():\n",
    "    df_imputed['ExperienceYearsAtThisCompany_log'] = boxcox(df_imputed['ExperienceYearsAtThisCompany_log'] + 1)[0]\n",
    "else:\n",
    "    df_imputed['ExperienceYearsAtThisCompany_log'] = boxcox(df_imputed['ExperienceYearsAtThisCompany_log'])[0]\n",
    "\n",
    "# Checking the new skewness values after transformations\n",
    "print(\"New skewness after transformation - 'YearsSinceLastPromotion':\", df_imputed['YearsSinceLastPromotion_log'].skew())\n",
    "print(\"New skewness after transformation - 'ExperienceYearsAtThisCompany':\", df_imputed['ExperienceYearsAtThisCompany_log'].skew())\n",
    "\n",
    "# Drop the original columns to avoid introducing highly correlated features\n",
    "df_imputed.drop(['YearsSinceLastPromotion', 'ExperienceYearsAtThisCompany'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f309ff-5540-41b1-baa2-648f9b4e0b03",
   "metadata": {},
   "source": [
    "### correlation plot with heatmap to detect and remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b8861-64b9-4065-8a23-f9c042107f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting our data using a heatmap\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(df_imputed.drop('EmpNumber',axis=1).corr(),annot=True,cmap='BuPu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db022fb-55bc-452d-afef-aaaa574fa930",
   "metadata": {},
   "source": [
    "##### No highly correlated features detected "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b9603-3a6a-4ed4-9476-3ee6a4bb1113",
   "metadata": {},
   "source": [
    "#### Feature selection using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e35754-44dd-4511-b252-915e92aa22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for modeling \n",
    "\n",
    "'''The code aims to standardize features in a dataset, fit a Lasso regression model for\n",
    "feature selection, and identify the most relevant features (up to 20) that contribute\n",
    "to predict the 'PerformanceRating' based on their impact and significance in the \n",
    "Lasso model. This process helps in building more efficient and potentially more\n",
    "accurate predictive models'''\n",
    "\n",
    "#Spleting data \n",
    "X = df_imputed.drop('PerformanceRating', axis=1)\n",
    "Y = df_imputed['PerformanceRating']\n",
    "\n",
    "# Create the pipeline with Lasso for feature selection\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectFromModel(Lasso(alpha=0.001), max_features=10, threshold=-np.inf))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the data\n",
    "pipeline.fit(X, Y)\n",
    "\n",
    "# Transform X using the pipeline to get a filtered dataset with selected features only\n",
    "X_selected = pipeline.transform(X)\n",
    "\n",
    "# Extract the names of selected features\n",
    "selected_features = X.columns[pipeline.named_steps['feature_selection'].get_support()]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Create a data frame with the selected features for further analysis, e.g., PCA\n",
    "X_filtered = pd.DataFrame(X_selected, columns=selected_features)\n",
    "X_filtered.head()  # Show the first few rows of the filtered DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e609cd-0cde-4cf6-848b-e00ab7964055",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e19bcd-b52d-42e3-9292-73b1ec5a49d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c0fabf-d64d-455b-bf1c-5ca4c0e91131",
   "metadata": {},
   "source": [
    "## 4. Model Building, traning and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e442c6b-0e15-4a29-9d7a-facfd089c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ede0d-8086-491d-860b-9f8a4a762786",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Balancing the data in the target variable so that it has an equal distribution\n",
    "Applying RandomOver_Sampler on our data to ensure that the target variables are evenly \n",
    "distributed thereby providing equal chances for each target class'''\n",
    "\n",
    "RandomOver_Sampler = RandomOverSampler(random_state=42)\n",
    "X_sampled,Y_sampled = RandomOver_Sampler.fit_resample(X_filtered, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c182b-7469-48b7-bebf-a4bfe8601ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the change in the distribution of classes in the target column after applying to the sample \n",
    "\n",
    "Y_sampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f6b8c-dd4b-4542-8e84-edfd8f67f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the pre_processed data\n",
    "\n",
    "X_sampled.to_csv('processed_data.csv', index=False)\n",
    "print(\"processed_data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e083918-acb5-4b1f-9d55-7a04383b6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637b17f-a35e-4389-a38f-f8d80dd13653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting our data into train tests to fit our model\n",
    "X_new = X_sampled\n",
    "Y_new = Y_sampled\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_new,Y_new,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bebda74-6eb4-4d4f-a58c-c15654fe5946",
   "metadata": {},
   "source": [
    "### Model selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa052f19-db29-4740-88d7-1e3974f6a659",
   "metadata": {},
   "source": [
    "###### Models to be used\n",
    "1. KNeighborsClassifier()\n",
    "2. RandomForestClassifier()\n",
    "3. SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70810ad6-9c7b-440f-b487-64266c292217",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Standardizing data ensures that each feature contributes equally to the analysis and improves \n",
    "the performance of machine learning algorithms by transforming features to have zero mean and \n",
    "unit variance, which facilitates faster convergence and more accurate model outcomes.'''\n",
    "\n",
    "# Initialize the StandardScaler for normalizing feature scales\n",
    "scaler_model = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it, ensuring all features contribute equally\n",
    "X_train = scaler_model.fit_transform(X_train)\n",
    "\n",
    "# Apply the same scaler to the test data for consistency\n",
    "X_test = scaler_model.transform(X_test)\n",
    "\n",
    "# Save the scaler model to a file for future use with the same data transformations\n",
    "dump(scaler_model, 'scaler_model.joblib')\n",
    "print(\"scaler_model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf6589-189d-4560-9cee-78aeaddd9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of models to be used in our model training function \n",
    "\n",
    "models = [KNeighborsClassifier(),RandomForestClassifier(),SVC()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f559f-bf53-4c21-9bd1-ef2db2c62285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_cross_validation(models, X_train, Y_train, X_test, Y_test):\n",
    "    for model in models:\n",
    "        # Perform cross-validation\n",
    "        cross_validation_scores = cross_val_score(model, X_train, Y_train, cv=5)\n",
    "        mean_cross_validation_score = round(cross_validation_scores.mean(), 3)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        # Predict on training and testing data\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate training and testing accuracies\n",
    "        train_accuracy = round(accuracy_score(Y_train, train_pred), 3)\n",
    "        test_accuracy = round(accuracy_score(Y_test, test_pred), 3)\n",
    "        \n",
    "        # Generate classification report for testing data\n",
    "        report = classification_report(Y_test, test_pred)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Model:\", type(model).__name__)\n",
    "        print(\"Mean Cross Validation Accuracy Score:\", mean_cross_validation_score)\n",
    "        print(\"Training Accuracy:\", train_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        print(\"=\"*50)\n",
    "compare_models_cross_validation(models, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352dc15f-c985-4ccd-a455-1eff1145fda3",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe7f4a-b865-4f0b-8c5f-8159bf77fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''The script conducts parameter optimization for various machine learning models using GridSearchCV to identify the most effective\n",
    "parameters based on accuracy. Results are collected and presented in a DataFrame for straightforward comparison and analysis.'''\n",
    "\n",
    "# Define a list of tuples containing model names, model instances, and dictionaries of hyperparameters for grid search\n",
    "models = [\n",
    "    ('KNN', KNeighborsClassifier(), {'n_neighbors': [3, 5, 10]}),\n",
    "    ('RandomForest', RandomForestClassifier(), {'n_estimators': [10, 20, 50, 100]}),\n",
    "    ('SVC', SVC(), {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [1, 5, 10, 20]})\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the results of the Grid Search for each model\n",
    "results = []\n",
    "\n",
    "# Iterate through each model and its associated parameters\n",
    "for name, model, params in models:\n",
    "    # Initialize GridSearchCV with the model and its parameters, specifying the number of folds and the scoring metric\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='accuracy')\n",
    "    # Fit GridSearchCV to the training data\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    # Extract the highest cross-validated score and the corresponding best parameters from the grid search\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Store the model name, best score, and best parameters in the results list\n",
    "    results.append({'Model': name, 'Best_Score': best_score, 'Best_Params': best_params})\n",
    "\n",
    "# Convert the list of results into a pandas DataFrame for better visualization and analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Output the results DataFrame showing the best model performance and parameters found\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3d24e-0693-43bb-b383-676b79c45bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dfbc154-e316-483e-84b1-90e79b43eaf1",
   "metadata": {},
   "source": [
    "### RandomForest model was selected for training \n",
    "\n",
    "Based on the results displayed in the DataFrame, the RandomForest model achieved the highest accuracy with a score of 0.974253 using 100 estimators, making it the most effective among the tested models for this dataset. Although the SVC model also shows strong performance with a 0.941830 accuracy using C=20 and the 'rbf' kernel, the RandomForest model stands out as the preferred choice due to its superior accuracy and effective handling of the features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6b994-323a-4876-8bd0-c54c5e60b6f4",
   "metadata": {},
   "source": [
    "### Training our selected model RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8fb1a-50ea-43d3-ad47-d2c1e5bcc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "\n",
    "#rf_model = SVC()\n",
    "\n",
    "# converting our data to the array\n",
    "\n",
    "X_train=np.asarray(X_train)\n",
    "#Y_train=np.asarray(X_train_scaled)\n",
    "# Train the model on the scaled training data\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the training data and the test data\n",
    "Y_train_pred = rf_model.predict(X_train)\n",
    "Y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy on the training data and the test data\n",
    "train_accuracy = accuracy_score(Y_train, Y_train_pred)\n",
    "test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "# Display the training and testing accuracy\n",
    "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "# Generate and display the classification report for the test data\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "\n",
    "print(\":\"*50)\n",
    "dump(rf_model, 'rf_model.joblib')\n",
    "print(\"rf_model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8253f5-dd11-4ced-a879-2177adf1cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1e677-3f54-47f9-99d4-7afabd177f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_test, Y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(Y_train), yticklabels=np.unique(Y_train))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9408df3d-9602-4159-9c77-fa93d2a8e74c",
   "metadata": {},
   "source": [
    "## 5. Results and Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5559710-8b2c-460f-b7d2-597406a21624",
   "metadata": {},
   "source": [
    "Summary of Project Insights and Recommendations:\n",
    "\n",
    "\n",
    "The project undertaken to analyze employee performance at INX Future Inc yielded several critical insights, structured around departmental performance, key performance-influencing factors, and the development of a predictive model. Here are the summarized outcomes:\n",
    "\n",
    "1. Department-wise Performances:\n",
    "Analysis revealed variable performance across different departments. The Data Science department outperformed others in terms of average employee performance, whereas departments like Finance lagged behind. Such disparities suggest department-specific dynamics and possibly varying management effectiveness or resource allocation.\n",
    "\n",
    "\n",
    "2. Top 3 Important Factors Affecting Employee Performance:\n",
    "\n",
    "Salary and Compensation: Employees receiving competitive and frequent salary increments tended to exhibit higher performance levels. This factor stood out as a significant motivator. \n",
    "\n",
    "Career Progression Opportunities: The duration since the last promotion was inversely related to performance, highlighting the importance of timely recognition and career advancement.\n",
    "\n",
    "Workplace Environment: Higher satisfaction levels in the workplace environment correlated directly with improved performance, indicating the critical role of work conditions and organizational culture.\n",
    "\n",
    "3. Predictive Model for Employee Performance:\n",
    "A machine learning model was developed to predict future employee performance based on the identified influential factors. This model uses inputs such as salary increments, promotion history, departmental data, and workplace satisfaction scores. The model can assist HR in making informed hiring decisions by predicting potential performance outcomes based on prospective employees’ profiles.\n",
    "\n",
    "\n",
    "4. Recommendations to Improve Employee Performance:\n",
    "\n",
    "Tailored Compensation Plans: Implement dynamic, performance-linked salary structures to continuously motivate high performance.\n",
    "\n",
    "Frequent and Transparent Promotion Cycles: Establish clear criteria for promotions and maintain a regular schedule for performance reviews to ensure employees feel recognized and valued.\n",
    "\n",
    "Enhance Workplace Environment: Invest in regular assessments and improvements to the working environment, including both physical and cultural aspects, to boost morale and productivity.\n",
    "\n",
    "Use of Predictive Analytics in Hiring: Integrate the predictive model into the recruitment process to select candidates whose profiles match the characteristics of high performers, thereby enhancing the overall quality of new hire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae660a7a-ab42-4048-a43a-c07365878d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_2.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb822b7-4eb1-4d73-862c-a55a0006fad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadd068-a22a-46f9-822d-cf2baa73252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.predict(X_new.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdb2f7-2cc8-480c-8091-d16a7d37dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting system\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load models and mappings\n",
    "mappings = joblib.load(r\"C:\\Users\\Muku\\OneDrive\\Desktop\\Data Science\\IABAC Exams\\IABAC_Projetc_Exam\\frequency_mappings.joblib\")\n",
    "scaler = joblib.load(r\"C:\\Users\\Muku\\OneDrive\\Desktop\\Data Science\\IABAC Exams\\IABAC_Projetc_Exam\\scaler_model.joblib\")\n",
    "model = joblib.load(r\"C:\\Users\\Muku\\OneDrive\\Desktop\\Data Science\\IABAC Exams\\IABAC_Projetc_Exam\\rf_model.joblib\")\n",
    "\n",
    "#\n",
    "emp_job_role= input('Employ role')\n",
    "distance_from_home = int(input('Distance From Home'))\n",
    "emp_education_level = int(input('Employee Education Level'))\n",
    "emp_environment_satisfaction = int(input('Employee Environment Satisfaction'))\n",
    "emp_hourly_rate = int(input('Employee Hourly Rate'))\n",
    "emp_last_salary_hike_percent = int(input('Employee Last Salary Hike Percent'))\n",
    "emp_work_life_balance = int(input('Employee Work Life Balance'))\n",
    "experience_years_in_current_role = int(input('Experience Years in Current Role'))\n",
    "years_with_curr_manager = int(input('Years with Current Manager'))\n",
    "years_since_last_promotion_log = int(input('Years Since Last Promotion (Log)'))\n",
    "\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "        'EmpJobRole': [emp_job_role],\n",
    "        'DistanceFromHome': [distance_from_home],\n",
    "        'EmpEducationLevel': [emp_education_level],\n",
    "        'EmpEnvironmentSatisfaction': [emp_environment_satisfaction],\n",
    "        'EmpHourlyRate': [emp_hourly_rate],\n",
    "        'EmpLastSalaryHikePercent': [emp_last_salary_hike_percent],\n",
    "        'EmpWorkLifeBalance': [emp_work_life_balance],\n",
    "        'ExperienceYearsInCurrentRole': [experience_years_in_current_role],\n",
    "        'YearsWithCurrManager': [years_with_curr_manager],\n",
    "        'YearsSinceLastPromotion_log': [years_since_last_promotion_log]\n",
    "    })\n",
    "new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb9602-7aa9-4eb0-bff8-4cb0aebc94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical columns using the loaded frequency mappings\n",
    "for column, mapping in mappings.items():\n",
    "    if column in new_data.columns:\n",
    "        new_data[column] = new_data[column].map(mapping).fillna(0)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f79d1-8ef6-40d5-bdde-894f9c7618d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d1c76-e7d4-4a40-91cb-95164aa6b3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d845956-3b95-4f35-b8cd-a340dd755c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical columns using the loaded frequency mappings\n",
    "for column, mapping in mappings.items():\n",
    "    if column in new_data.columns:\n",
    "        new_data[column] = new_data[column].map(mapping).fillna(0)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Standardize data\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "new_data_scaled\n",
    "# Make prediction\n",
    "prediction = model.predict(new_data_scaled)\n",
    "print(f'PerformanceRating for this employee is: {prediction[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab0b71-77e4-4a1b-bd11-9a90cbef7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_scaled = scaler.transform(new_data)\n",
    "new_data_scaled\n",
    "# Make prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564b0c1-1bfe-4b7e-9139-b318e3a07610",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = np.asarray([ 0.22624275,  4.91238686,  3.93583652,  4.13663525, 47.26712072,\n",
    "        17.42372426,  2.89465959, 12.82481305, 11.54331933,  0.89728975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b765eda-0e8d-4d3d-a6a7-d2ad8037f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_new.sample(1))\n",
    "prediction = model.predict(X_new.sample(1))\n",
    "print(f'PerformanceRating for this employee is: {prediction[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95dd56-5d35-447a-872d-fae5d18d0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf373fd7-0c84-4198-8c5e-1ddd26a91f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e67d0-31f9-4341-9854-1fe00f8274b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
